import re
import os
from datetime import datetime, timedelta
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from janome.tokenizer import Tokenizer
from wordcloud import WordCloud
from collections import Counter
from matplotlib.ticker import FuncFormatter

# --- å®šæ•° ---
DATA_FOLDER = "data"
OUTPUT_BASE_FOLDER = "output"
TODAY_STR = datetime.now().strftime("%Y-%m-%d")
OUTPUT_FOLDER = os.path.join(OUTPUT_BASE_FOLDER, TODAY_STR)
os.makedirs(OUTPUT_FOLDER, exist_ok=True)

EXCLUDED_POS = {"åŠ©è©", "åŠ©å‹•è©", "è¨˜å·", "æ¥ç¶šè©", "é€£ä½“è©", "æ„Ÿå‹•è©", "ãƒ•ã‚£ãƒ©ãƒ¼", "ãã®ä»–"}
EXCLUDED_WORDS = {"ã™ã‚‹", "ã‚Œã‚‹", "ã‚‰ã‚Œã‚‹", "ãªã‚‹",
                  "ã‚ã‚‹", "ã“ã¨", "ã‚‚ã®", "ã¦ã‚‹", "ãã‚‹", "ã„ã‚‹", "ã§ãã‚‹"}
WEEKDAYS_EN = ["Monday", "Tuesday", "Wednesday",
               "Thursday", "Friday", "Saturday", "Sunday"]
WEEKDAYS_JP = ["æœˆæ›œ", "ç«æ›œ", "æ°´æ›œ", "æœ¨æ›œ", "é‡‘æ›œ", "åœŸæ›œ", "æ—¥æ›œ"]

# --- ãƒ•ã‚©ãƒ³ãƒˆè¨­å®š ---
plt.rcParams['font.family'] = 'Meiryo' if os.name == 'nt' else 'IPAexGothic'


# --- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° ---
def comma_formatter(x, pos):
    return f'{int(x):,}'


def save_figure(filename):
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_FOLDER, filename))
    plt.close()


def get_channel_name(file):
    return os.path.splitext(file)[0]


def load_channel_files(folder=DATA_FOLDER):
    return [f for f in os.listdir(folder) if f.endswith(".csv")]


def read_data(filepath):
    df = pd.read_csv(filepath)
    df["published_at"] = pd.to_datetime(
        df["published_at"]).dt.tz_convert("Asia/Tokyo")
    return df


def extract_meaningful_words(text, tokenizer):
    text = re.sub(r'ã€[^ã€‘]*ã€‘', '', text)
    text = re.sub(r'#short', '', text, flags=re.IGNORECASE)
    for word in ["ã‚¹ã‚«ãƒƒã¨è¿·è¨€é›†", "è¿·è¨€é›†", "ch", "æ„Ÿå‹•", "ã‚¹ãƒ¬", "ww", "ã‚¹ã‚«ãƒƒ", "10", "#"]:
        text = text.replace(word, "")
    text = text.strip()

    words = []
    for token in tokenizer.tokenize(text):
        pos = token.part_of_speech.split(',')[0]
        base = token.base_form
        if pos not in EXCLUDED_POS and base not in EXCLUDED_WORDS and len(base) > 1:
            words.append(base)
    return words


# --- ãƒãƒ£ãƒ³ãƒãƒ«åˆ¥çµ±è¨ˆ ---
def summarize_channels(files):
    summary = []
    for file in files:
        df = read_data(os.path.join(DATA_FOLDER, file))
        summary.append({
            "channel": get_channel_name(file),
            "video_count": len(df),
            "average_views": round(df["viewCount"].mean())
        })
    return pd.DataFrame(summary)


def plot_channel_summary(df):
    plt.figure(figsize=(10, 6))
    sns.barplot(data=df, x="channel", y="video_count", palette="Greens_d")
    plt.title("ãƒãƒ£ãƒ³ãƒãƒ«åˆ¥ãƒ»æŠ•ç¨¿æœ¬æ•°")
    plt.xlabel("ãƒãƒ£ãƒ³ãƒãƒ«å")
    plt.ylabel("æŠ•ç¨¿æœ¬æ•°")
    plt.xticks(rotation=30, ha='right')
    save_figure("ãƒãƒ£ãƒ³ãƒãƒ«åˆ¥_æŠ•ç¨¿æœ¬æ•°.png")


# --- æ›œæ—¥ãƒ»æ™‚é–“å¸¯åˆ¥ ---
def analyze_short_videos_by_weekday_hour(files):
    weekday_dfs, hour_dfs = [], []
    for file in files:
        df = pd.read_csv(os.path.join(DATA_FOLDER, file))
        if "is_short" not in df.columns:
            continue
        df = df[df["is_short"]]
        if df.empty:
            continue
        df["published_at"] = pd.to_datetime(df["published_at"])
        df["weekday"] = df["published_at"].dt.day_name()
        df["hour"] = df["published_at"].dt.hour
        df["channel"] = get_channel_name(file)
        weekday_dfs.append(df[["channel", "weekday", "viewCount"]])
        hour_dfs.append(df[["channel", "hour", "viewCount"]])
    return weekday_dfs, hour_dfs


def plot_weekday_views(weekday_dfs):
    if not weekday_dfs:
        return
    df = pd.concat(weekday_dfs)
    avg_views = df.groupby("weekday")["viewCount"].mean().reindex(WEEKDAYS_EN)
    avg_views.index = WEEKDAYS_JP
    plt.figure(figsize=(10, 6))
    sns.barplot(x=avg_views.index, y=avg_views.values, palette="coolwarm")
    plt.title("æ›œæ—¥åˆ¥ãƒ»å¹³å‡å†ç”Ÿæ•°ï¼ˆã‚·ãƒ§ãƒ¼ãƒˆå‹•ç”»ï¼‰")
    plt.xlabel("æ›œæ—¥")
    plt.ylabel("å¹³å‡å†ç”Ÿæ•°")
    plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_formatter))
    save_figure("æ›œæ—¥åˆ¥_å¹³å‡å†ç”Ÿæ•°_ã‚·ãƒ§ãƒ¼ãƒˆå‹•ç”».png")


def plot_hourly_views(hour_dfs):
    if not hour_dfs:
        return
    df = pd.concat(hour_dfs)
    avg_views = df.groupby("hour")["viewCount"].mean()
    plt.figure(figsize=(10, 6))
    sns.lineplot(x=avg_views.index, y=avg_views.values, marker="o")
    plt.title("æ™‚é–“å¸¯åˆ¥ãƒ»å¹³å‡å†ç”Ÿæ•°ï¼ˆã‚·ãƒ§ãƒ¼ãƒˆå‹•ç”»ï¼‰")
    plt.xlabel("æ™‚é–“ï¼ˆæ™‚ï¼‰")
    plt.ylabel("å¹³å‡å†ç”Ÿæ•°")
    plt.xticks(range(24))
    plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_formatter))
    save_figure("æ™‚é–“å¸¯åˆ¥_å¹³å‡å†ç”Ÿæ•°_ã‚·ãƒ§ãƒ¼ãƒˆå‹•ç”».png")


# --- Top10å‹•ç”» ---
def get_top10_videos_this_week(files):
    now = pd.Timestamp.now(tz="Asia/Tokyo")
    one_week_ago = now - timedelta(days=7)
    top_videos = []
    for file in files:
        df = read_data(os.path.join(DATA_FOLDER, file))
        df = df[df["published_at"] >= one_week_ago]
        for _, row in df.iterrows():
            top_videos.append({
                "channel": get_channel_name(file),
                "title": row["title"],
                "viewCount": row["viewCount"],
                "published_at": row["published_at"]
            })
    top_df = pd.DataFrame(top_videos)
    return top_df.sort_values("viewCount", ascending=False).head(10)


def save_top10(top10_df):
    top10_df.to_csv(os.path.join(OUTPUT_FOLDER, "ä»Šé€±ã®Top10å‹•ç”».csv"), index=False)


# --- ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ— ---
def plot_heatmap_weekday_hour(files):
    all_dfs = []
    for file in files:
        df = read_data(os.path.join(DATA_FOLDER, file))
        df["weekday"] = df["published_at"].dt.dayofweek
        df["hour"] = df["published_at"].dt.hour
        all_dfs.append(df)

    combined = pd.concat(all_dfs)

    # ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
    heatmap_data = combined.pivot_table(
        index="hour",
        columns="weekday",
        values="viewCount",
        aggfunc="mean"
    )

    # æ—¥æœ¬èªã®æ›œæ—¥ã«å¤‰æ›ï¼ˆæœˆã€œæ—¥ï¼‰
    WEEKDAYS_JP = ["æœˆ", "ç«", "æ°´", "æœ¨", "é‡‘", "åœŸ", "æ—¥"]
    heatmap_data.columns = [WEEKDAYS_JP[day] for day in heatmap_data.columns]

    # --- ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—æç”» ---
    plt.figure(figsize=(12, 8))
    sns.heatmap(
        heatmap_data,
        cmap="coolwarm",
        linewidths=0.5,
        linecolor='gray',
        annot=True,
        fmt=',.0f'
    )

    plt.title("æ›œæ—¥ Ã— æ™‚é–“å¸¯åˆ¥ã®å¹³å‡å†ç”Ÿæ•°\nï¼ˆè¦–è´å‚¾å‘ã‚’å¯è¦–åŒ–ï¼‰", fontsize=16)
    plt.xlabel("æ›œæ—¥", fontsize=12)
    plt.ylabel("æ™‚é–“å¸¯ï¼ˆæ™‚ï¼‰", fontsize=12)
    plt.xticks(fontsize=10)
    plt.yticks(fontsize=10)

    plt.tight_layout()

    # ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«åã‚‚ã‚ã‹ã‚Šã‚„ã™ã
    save_figure("æ›œæ—¥åˆ¥_æ™‚é–“åˆ¥_å¹³å‡å†ç”Ÿæ•°_ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—.png")


# --- ãƒãƒ£ãƒ³ãƒãƒ«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ ---
def analyze_channel_performance(files):
    performances = []
    all_videos = []
    for file in files:
        df = read_data(os.path.join(DATA_FOLDER, file))
        df["week"] = df["published_at"].dt.strftime("%Y-%U")
        df["channel"] = get_channel_name(file)
        all_videos.append(df)
        performances.append({
            "channel": df["channel"].iloc[0],
            "avg_views": round(df["viewCount"].mean()),
            "freq_per_week": round(len(df) / df["week"].nunique(), 2)
        })
    combined = pd.concat(all_videos)
    overall_avg = combined["viewCount"].mean()
    for perf in performances:
        ch_df = combined[combined["channel"] == perf["channel"]]
        success_rate = (ch_df["viewCount"] > overall_avg).sum() / len(ch_df)
        perf["success_rate"] = round(success_rate * 100, 1)
    return pd.DataFrame(performances)


def plot_channel_performance(df):
    for col, title, color, ylabel in [
        ("avg_views", "ãƒãƒ£ãƒ³ãƒãƒ«åˆ¥ãƒ»å¹³å‡å†ç”Ÿæ•°", "Blues_d", "å¹³å‡å†ç”Ÿæ•°"),
        ("freq_per_week", "ãƒãƒ£ãƒ³ãƒãƒ«åˆ¥ãƒ»æŠ•ç¨¿é »åº¦ï¼ˆé€±ã‚ãŸã‚Šï¼‰", "Greens_d", "æŠ•ç¨¿é »åº¦"),
        ("success_rate", "ãƒãƒ£ãƒ³ãƒãƒ«åˆ¥ãƒ»æˆåŠŸç‡ï¼ˆ%ï¼‰", "Purples_d", "æˆåŠŸç‡ï¼ˆ%ï¼‰")
    ]:
        plt.figure(figsize=(12, 6))
        sns.barplot(data=df, x="channel", y=col, palette=color)
        plt.title(title)
        plt.xlabel("ãƒãƒ£ãƒ³ãƒãƒ«å")
        plt.ylabel(ylabel)
        if col == "avg_views":
            plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_formatter))
        plt.xticks(rotation=30)
        save_figure(f"{title}.png")


# --- å‹•ç”»ã®é•·ã•åˆ¥ ---
def analyze_views_by_duration(files):
    all_data = []
    for file in files:
        df = read_data(os.path.join(DATA_FOLDER, file))
        if "duration" not in df.columns:
            continue
        df = df.dropna(subset=["duration"])
        df["duration"] = df["duration"].astype(float)
        df["duration_cat"] = df["duration"].apply(lambda d: (
            "ï½15ç§’" if d <= 15 else
            "16ï½30ç§’" if d <= 30 else
            "31ï½45ç§’" if d <= 45 else
            "46ï½60ç§’" if d <= 60 else
            "60ç§’è¶…"
        ))
        all_data.append(df)
    if not all_data:
        print("å‹•ç”»ã®é•·ã•ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return
    combined = pd.concat(all_data)
    avg_views = combined.groupby("duration_cat")["viewCount"].mean().reindex(
        ["ï½15ç§’", "16ï½30ç§’", "31ï½45ç§’", "46ï½60ç§’", "60ç§’è¶…"]
    )
    plt.figure(figsize=(10, 6))
    sns.barplot(x=avg_views.index, y=avg_views.values, palette="magma")
    plt.title("å‹•ç”»ã®é•·ã•åˆ¥ãƒ»å¹³å‡å†ç”Ÿæ•°")
    plt.xlabel("å‹•ç”»ã®é•·ã•")
    plt.ylabel("å¹³å‡å†ç”Ÿæ•°")
    plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_formatter))
    save_figure("å‹•ç”»é•·ã•åˆ¥_å¹³å‡å†ç”Ÿæ•°.png")


# --- ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ ---
def generate_wordcloud(files):
    tokenizer = Tokenizer()
    words = []
    for file in files:
        df = pd.read_csv(os.path.join(DATA_FOLDER, file))
        if "is_short" not in df.columns:
            continue
        df = df[df["is_short"]]
        for title in df["title"].dropna().astype(str):
            words.extend(extract_meaningful_words(title, tokenizer))
    if words:
        wc = WordCloud(font_path="C:/Windows/Fonts/meiryo.ttc",
                       background_color="white", width=800, height=600).generate(" ".join(words))
        plt.figure(figsize=(10, 8))
        plt.imshow(wc, interpolation="bilinear")
        plt.axis("off")
        save_figure("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰.png")


# --- æˆåŠŸå‹•ç”»ãƒ¯ãƒ¼ãƒ‰ ---
def analyze_success_title_words(files):
    tokenizer = Tokenizer()
    all_dfs = []
    for file in files:
        df = read_data(os.path.join(DATA_FOLDER, file))
        df["channel"] = get_channel_name(file)
        all_dfs.append(df)
    combined = pd.concat(all_dfs)
    overall_avg_views = combined["viewCount"].mean()
    success_videos = combined[combined["viewCount"] >= overall_avg_views]
    if success_videos.empty:
        print("æˆåŠŸå‹•ç”»ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return
    words = []
    for title in success_videos["title"].dropna().astype(str):
        words.extend(extract_meaningful_words(title, tokenizer))
    if not words:
        print("æˆåŠŸå‹•ç”»ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰æŠ½å‡ºã§ãã‚‹å˜èªãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return
    word_counts = Counter(words)
    common_words = word_counts.most_common(20)
    words_list, counts = zip(*common_words)
    plt.figure(figsize=(12, 6))
    sns.barplot(x=list(counts), y=list(words_list), palette="viridis")
    plt.title("æˆåŠŸå‹•ç”»ã‚¿ã‚¤ãƒˆãƒ«é »å‡ºãƒ¯ãƒ¼ãƒ‰ Top20")
    plt.xlabel("å‡ºç¾å›æ•°")
    plt.ylabel("å˜èª")
    save_figure("æˆåŠŸå‹•ç”»ã‚¿ã‚¤ãƒˆãƒ«é »å‡ºãƒ¯ãƒ¼ãƒ‰_Top20.png")


def export_all_to_excel(summary_df, top10_df, weekday_dfs, hour_dfs, perf_df):
    output_path = os.path.join(OUTPUT_FOLDER, "åˆ†æãƒ¬ãƒãƒ¼ãƒˆ.xlsx")

    # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã‚’é™¤å»ï¼ˆExcelå¯¾å¿œï¼‰
    if "published_at" in top10_df.columns:
        top10_df["published_at"] = top10_df["published_at"].dt.tz_localize(
            None)

    # ã‚«ãƒ©ãƒ åã‚’æ—¥æœ¬èªã«å¤‰æ›ï¼ˆtop10ï¼‰
    top10_df = top10_df.rename(columns={
        "channel": "ãƒãƒ£ãƒ³ãƒãƒ«å",
        "title": "ã‚¿ã‚¤ãƒˆãƒ«",
        "viewCount": "å†ç”Ÿæ•°",
        "published_at": "æŠ•ç¨¿æ—¥"
    })

    # æ›œæ—¥åˆ¥
    if weekday_dfs:
        weekday_df = pd.concat(weekday_dfs)
        weekday_avg = weekday_df.groupby(
            "weekday")["viewCount"].mean().reindex(WEEKDAYS_EN)
        weekday_avg.index = WEEKDAYS_JP
        weekday_avg_df = weekday_avg.reset_index()
        weekday_avg_df.columns = ["æ›œæ—¥", "å¹³å‡å†ç”Ÿæ•°"]
    else:
        weekday_avg_df = pd.DataFrame(columns=["æ›œæ—¥", "å¹³å‡å†ç”Ÿæ•°"])

    # æ™‚é–“å¸¯åˆ¥
    if hour_dfs:
        hour_df = pd.concat(hour_dfs)
        hour_avg = hour_df.groupby("hour")["viewCount"].mean().reset_index()
        hour_avg.columns = ["æ™‚é–“", "å¹³å‡å†ç”Ÿæ•°"]
    else:
        hour_avg = pd.DataFrame(columns=["æ™‚é–“", "å¹³å‡å†ç”Ÿæ•°"])

    # ãƒãƒ£ãƒ³ãƒãƒ«åˆ¥ã‚µãƒãƒªï¼ˆæŠ•ç¨¿æœ¬æ•°ãƒ»å¹³å‡å†ç”Ÿæ•°ï¼‰
    summary_df = summary_df.rename(columns={
        "channel": "ãƒãƒ£ãƒ³ãƒãƒ«å",
        "video_count": "æŠ•ç¨¿æœ¬æ•°",
        "average_views": "å¹³å‡å†ç”Ÿæ•°"
    })

    # ãƒãƒ£ãƒ³ãƒãƒ«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆå¹³å‡å†ç”Ÿæ•°ãƒ»æŠ•ç¨¿é »åº¦ãƒ»æˆåŠŸç‡ï¼‰
    perf_df = perf_df.rename(columns={
        "channel": "ãƒãƒ£ãƒ³ãƒãƒ«å",
        "avg_views": "å¹³å‡å†ç”Ÿæ•°",
        "freq_per_week": "æŠ•ç¨¿é »åº¦ï¼ˆé€±ã‚ãŸã‚Šï¼‰",
        "success_rate": "æˆåŠŸç‡ï¼ˆ%ï¼‰"
    })

    # æ›¸ãå‡ºã—
    with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:
        summary_df.to_excel(writer, index=False, sheet_name="ãƒãƒ£ãƒ³ãƒãƒ«åˆ¥ã‚µãƒãƒª")
        top10_df.to_excel(writer, index=False, sheet_name="ä»Šé€±ã®Top10å‹•ç”»")
        weekday_avg_df.to_excel(writer, index=False,
                                sheet_name="æ›œæ—¥åˆ¥å¹³å‡å†ç”Ÿæ•°ï¼ˆã‚·ãƒ§ãƒ¼ãƒˆï¼‰")
        hour_avg.to_excel(writer, index=False, sheet_name="æ™‚é–“å¸¯åˆ¥å¹³å‡å†ç”Ÿæ•°ï¼ˆã‚·ãƒ§ãƒ¼ãƒˆï¼‰")
        perf_df.to_excel(writer, index=False, sheet_name="ãƒãƒ£ãƒ³ãƒãƒ«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ")


# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
def main():
    files = load_channel_files()
    summary_df = summarize_channels(files)
    plot_channel_summary(summary_df)
    weekday_dfs, hour_dfs = analyze_short_videos_by_weekday_hour(files)
    plot_weekday_views(weekday_dfs)
    plot_hourly_views(hour_dfs)
    generate_wordcloud(files)
    top10_df = get_top10_videos_this_week(files)
    save_top10(top10_df)
    plot_heatmap_weekday_hour(files)
    perf_df = analyze_channel_performance(files)
    plot_channel_performance(perf_df)
    analyze_views_by_duration(files)
    analyze_success_title_words(files)
    export_all_to_excel(summary_df, top10_df, weekday_dfs, hour_dfs, perf_df)


if __name__ == "__main__":
    print("ğŸ“Š YouTubeãƒ‡ãƒ¼ã‚¿åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
    main()
